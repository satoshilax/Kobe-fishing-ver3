#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Á•ûÊà∏Èá£„ÇäÊÉÖÂ†± Ëá™Âãï„Éá„Éº„ÇøÂèéÈõÜ„Çπ„ÇØ„É™„Éó„Éà v2.0
5„Å§„ÅÆ„ÇΩ„Éº„Çπ„Åã„ÇâÂÆüÈöõ„ÅÆÈá£Êûú„Éá„Éº„Çø„ÇíÂèéÈõÜ
"""

import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
import time
import math

# „É™„ÇØ„Ç®„Çπ„Éà„Éò„ÉÉ„ÉÄ„Éº
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
}

# È≠öÁ®Æ„ÅÆÁµµÊñáÂ≠ó„Éû„ÉÉ„Éî„É≥„Ç∞
FISH_EMOJI = {
    '„Ç¢„Ç∏': 'üê†', '„Çµ„Éê': 'üêü', '„É°„Éê„É´': 'üêü', '„Çø„ÉÅ„Ç¶„Ç™': 'üó°Ô∏è',
    '„Çø„Ç≥': 'üêô', '„Ç§„Ç´': 'ü¶ë', '„Ç¢„Ç™„É™„Ç§„Ç´': 'ü¶ë', '„ÉÅ„Éå': 'üêü',
    '„Ç≠„Çπ': 'üê†', '„Ç´„É¨„Ç§': 'üêü', '„Éè„Çº': 'üê†', '„Ç´„Çµ„Ç¥': 'üêü',
    '„Ç¨„Ç∑„É©': 'üêü', '„Ç∑„Éº„Éê„Çπ': 'üêü', '„Éè„Éû„ÉÅ': 'üêü', '„Çµ„ÉØ„É©': 'üêü',
    '„Éû„ÉÄ„Ç§': 'üé£', '„Ç§„ÉØ„Ç∑': 'üê†', '„Éè„Éç': 'üêü', '„Ç∞„É¨': 'üêü',
    '„Çµ„É®„É™': 'üê†', '„Ç¶„Éü„Çø„Éä„Ç¥': 'üêü', '„Ç≥„Éñ„ÉÄ„Ç§': 'üêü', '„Éï„Ç∞': 'üê°',
    '„Ç¶„Éû„ÉÖ„É©„Éè„ÇÆ': 'üêü', '„Ç´„ÉØ„Éè„ÇÆ': 'üêü', '„ÉÑ„Éê„Çπ': 'üêü', '„Éñ„É™': 'üêü',
    '„Çπ„Ç∫„Ç≠': 'üêü', '„Ç¢„Ç§„Éä„É°': 'üêü', '„Éí„É©„É°': 'üêü', '„Éô„É©': 'üê†',
    '„Ç¶„Éä„ÇÆ': 'üêç', '„Ç¢„Éä„Ç¥': 'üêç', '„Çµ„É≥„Éê„ÇΩ„Ç¶': 'üêü',
}

def get_emoji(fish_name):
    """È≠öÂêç„Åã„ÇâÁµµÊñáÂ≠ó„ÇíÂèñÂæó"""
    for key, emoji in FISH_EMOJI.items():
        if key in fish_name:
            return emoji
    return 'üêü'


# =============================================================
# 1. È†àÁ£®Êµ∑„Å•„ÇäÂÖ¨Âúí
# =============================================================
def collect_suma():
    """È†àÁ£®Êµ∑„Å•„ÇäÂÖ¨Âúí„Åã„ÇâÈá£ÊûúÂèéÈõÜ"""
    print("üì° [1/5] È†àÁ£®Êµ∑„Å•„ÇäÂÖ¨Âúí„Åã„ÇâÂèéÈõÜ‰∏≠...")
    catches = []
    
    try:
        # Èá£Êûú‰∏ÄË¶ß„Éö„Éº„Ç∏ÂèñÂæó
        url = "https://sumasakana-park.com/fishing/"
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'lxml')
        
        # ÂêÑÊó•„ÅÆÈá£Êûú„É™„É≥„ÇØ„ÇíÂèñÂæóÔºàÊúÄÊñ∞5‰ª∂Ôºâ
        articles = soup.select('a[href*="/fishing/"]')
        detail_urls = []
        for a in articles:
            href = a.get('href', '')
            if href and '/fishing/' in href and href != '/fishing/' and 'page' not in href:
                full_url = href if href.startswith('http') else f"https://sumasakana-park.com{href}"
                if full_url not in detail_urls:
                    detail_urls.append(full_url)
        
        detail_urls = detail_urls[:7]  # ÊúÄÊñ∞7Êó•ÂàÜ
        
        for detail_url in detail_urls:
            try:
                time.sleep(1)  # Á§ºÂÑÄÊ≠£„Åó„Åè
                resp2 = requests.get(detail_url, headers=HEADERS, timeout=15)
                resp2.encoding = 'utf-8'
                soup2 = BeautifulSoup(resp2.text, 'lxml')
                
                # Êó•‰ªòÂèñÂæó
                title = soup2.find('h2', string=re.compile(r'20\d{2}\.\d{2}\.\d{2}'))
                date_str = ""
                if title:
                    m = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', title.text)
                    if m:
                        date_str = f"{int(m.group(2))}/{int(m.group(3))}"
                
                # Â§©ÂÄô„ÉªÊ∞¥Ê∏©ÂèñÂæó
                water_temp = ""
                tide = ""
                weather = ""
                for li in soup2.select('li'):
                    text = li.get_text(strip=True)
                    if 'Ê∞¥Ê∏©' in text:
                        m = re.search(r'([\d.]+)‚ÑÉ', text)
                        if m:
                            water_temp = m.group(1) + "‚ÑÉ"
                    if 'ÊΩÆ' in text and 'Ê∫ÄÊΩÆ' not in text and 'Âπ≤ÊΩÆ' not in text:
                        for s in ['Â§ßÊΩÆ', '‰∏≠ÊΩÆ', 'Â∞èÊΩÆ', 'Èï∑ÊΩÆ', 'Ëã•ÊΩÆ']:
                            if s in text:
                                tide = s
                                break
                    if any(w in text for w in ['Êô¥„Çå', 'Êõá„Çä', 'Èõ®', 'Êô¥']):
                        for w in ['Êô¥„Çå', 'Êõá„ÇäÊôÇ„ÄÖÈõ®', 'Êõá„ÇäÊôÇ„ÄÖÊô¥„Çå', 'Êõá„Çä', 'Èõ®„ÅÆ„Å°Êõá„Çä', 'Èõ®', 'Êô¥']:
                            if w in text:
                                weather = w
                                break
                
                # Èá£Êûú„ÉÜ„Éº„Éñ„É´ÂèñÂæó
                tables = soup2.select('table')
                for table in tables:
                    rows = table.select('tr')
                    for row in rows:
                        cells = row.select('td')
                        if len(cells) >= 3:
                            fish = cells[0].get_text(strip=True)
                            size = cells[1].get_text(strip=True)
                            count = cells[2].get_text(strip=True)
                            if fish and any(c.isalpha() or ord(c) > 127 for c in fish):
                                catches.append({
                                    "fish": fish,
                                    "size": size,
                                    "count": count,
                                    "method": "",
                                    "user": "",
                                    "date": date_str,
                                    "emoji": get_emoji(fish),
                                    "water_temp": water_temp,
                                    "tide": tide,
                                    "weather": weather,
                                })
                
                # „ÉÜ„Éº„Éñ„É´„Åå„Å™„ÅÑÂ†¥Âêà„ÄÅÊú¨Êñá„Åã„Çâ„ÇÇÊäΩÂá∫„ÇíË©¶„Åø„Çã
                if not any(c['date'] == date_str for c in catches):
                    content = soup2.get_text()
                    # "Èá£Êûú„Å™„Åó" „Éë„Çø„Éº„É≥
                    if 'Èá£Êûú„Å™„Åó' in content or 'ÁõÆÁ´ã„Å£„ÅüÈá£Êûú„Å™„Åó' in content:
                        catches.append({
                            "fish": "Èá£Êûú„Å™„Åó",
                            "size": "-",
                            "count": "-",
                            "method": "",
                            "user": "",
                            "date": date_str,
                            "emoji": "‚ùå",
                            "water_temp": water_temp,
                            "tide": tide,
                            "weather": weather,
                        })
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è È†àÁ£®Ë©≥Á¥∞„Éö„Éº„Ç∏„Ç®„É©„Éº: {detail_url} - {e}")
                continue
        
    except Exception as e:
        print(f"  ‚ùå È†àÁ£®Êµ∑„Å•„ÇäÂÖ¨Âúí„Ç®„É©„Éº: {e}")
    
    print(f"  ‚úÖ È†àÁ£®: {len(catches)}‰ª∂ÂèñÂæó")
    return {
        "name": "È†àÁ£®Êµ∑„Å•„ÇäÂÖ¨Âúí",
        "area": "Á•ûÊà∏",
        "distance": 2.3,
        "info": "„Éï„Ç°„Éü„É™„ÉºÂêë„Åë„ÉªË∂≥Â†¥ËâØÂ•Ω„ÉªË®≠ÂÇôÂÖÖÂÆü„ÉªÈßêËªäÂ†¥„ÅÇ„Çä",
        "source": "sumasakana-park.com",
        "catches": catches
    }


# =============================================================
# 2. Âπ≥Á£ØÊµ∑„Å•„ÇäÂÖ¨Âúí
# =============================================================
def collect_hiraiso():
    """Âπ≥Á£ØÊµ∑„Å•„ÇäÂÖ¨Âúí„Åã„ÇâÈá£ÊûúÂèéÈõÜ"""
    print("üì° [2/5] Âπ≥Á£ØÊµ∑„Å•„ÇäÂÖ¨Âúí„Åã„ÇâÂèéÈõÜ‰∏≠...")
    catches = []
    
    try:
        url = "https://kobeumiduri.jp/fishresult/"
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'lxml')
        
        # ÊúàÈñìÈá£Êûú„ÉÜ„Éº„Éñ„É´ÂèñÂæó
        table = soup.select_one('table')
        if table:
            rows = table.select('tr')
            for row in rows[1:]:  # „Éò„ÉÉ„ÉÄ„Éº„Çπ„Ç≠„ÉÉ„Éó
                cells = row.select('td')
                if len(cells) >= 7:
                    fish = cells[0].get_text(strip=True)
                    rating = cells[1].get_text(strip=True)
                    size = cells[2].get_text(strip=True)
                    count = cells[3].get_text(strip=True)
                    method = cells[4].get_text(strip=True)
                    bait = cells[5].get_text(strip=True)
                    location = cells[6].get_text(strip=True)
                    
                    if fish and fish != 'È≠öÁ®Æ':
                        catches.append({
                            "fish": fish,
                            "size": size,
                            "count": count,
                            "method": method,
                            "user": f"„Ç®„Çµ:{bait}",
                            "date": "‰ªäÊúàÂÆüÁ∏æ",
                            "emoji": get_emoji(fish),
                            "rating": rating,
                            "location": location,
                        })
        
        # ÂÄãÂà•Èá£Êûú„Éö„Éº„Ç∏„ÅÆ„É™„É≥„ÇØÂèñÂæóÔºàÊúÄÊñ∞5‰ª∂Ôºâ
        result_links = []
        for a in soup.select('a[href*="fishresult"]'):
            href = a.get('href', '')
            if re.search(r'20\d{2}.*\d{1,2}.*\d{1,2}', href):
                full_url = href if href.startswith('http') else f"https://kobeumiduri.jp{href}"
                if full_url not in result_links:
                    result_links.append(full_url)
        
        for detail_url in result_links[:5]:
            try:
                time.sleep(1)
                resp2 = requests.get(detail_url, headers=HEADERS, timeout=15)
                resp2.encoding = 'utf-8'
                soup2 = BeautifulSoup(resp2.text, 'lxml')
                
                # Êó•‰ªòÂèñÂæó
                h2 = soup2.find('h2', string=re.compile(r'20\d{2}Âπ¥'))
                date_str = ""
                if h2:
                    m = re.search(r'(\d{1,2})Êúà(\d{1,2})Êó•', h2.text)
                    if m:
                        date_str = f"{m.group(1)}/{m.group(2)}"
                
                # Â§©ÂÄô„ÉªÊ∞¥Ê∏©
                water_temp = ""
                tide = ""
                page_text = soup2.get_text()
                m = re.search(r'Ê∞¥Ê∏©\s*([\d.]+)', page_text)
                if m:
                    water_temp = m.group(1) + "‚ÑÉ"
                for s in ['Â§ßÊΩÆ', '‰∏≠ÊΩÆ', 'Â∞èÊΩÆ', 'Èï∑ÊΩÆ', 'Ëã•ÊΩÆ']:
                    if s in page_text:
                        tide = s
                        break
                
                # ÂÄãÂà•Èá£Êûú„ÅÆË©≥Á¥∞
                fish_name = ""
                size_val = ""
                count_val = ""
                method_val = ""
                bait_val = ""
                
                for text_block in page_text.split('\n'):
                    text_block = text_block.strip()
                    if 'È≠öÁ®Æ' in text_block:
                        m = re.search(r'È≠öÁ®Æ\s*(.+)', text_block)
                        if m: fish_name = m.group(1).strip()
                    elif '„Çµ„Ç§„Ç∫' in text_block:
                        m = re.search(r'„Çµ„Ç§„Ç∫\s*(.+)', text_block)
                        if m: size_val = m.group(1).strip()
                    elif 'Â∞æÊï∞' in text_block:
                        m = re.search(r'Â∞æÊï∞\s*(.+)', text_block)
                        if m: count_val = m.group(1).strip()
                    elif '‰ªïÊéõ' in text_block:
                        m = re.search(r'‰ªïÊéõ\s*(.+)', text_block)
                        if m: method_val = m.group(1).strip()
                    elif '„Ç®„Çµ' in text_block:
                        m = re.search(r'„Ç®„Çµ\s*(.+)', text_block)
                        if m: bait_val = m.group(1).strip()
                
                if fish_name:
                    catches.append({
                        "fish": fish_name,
                        "size": size_val,
                        "count": count_val,
                        "method": method_val,
                        "user": f"„Ç®„Çµ:{bait_val}" if bait_val else "",
                        "date": date_str,
                        "emoji": get_emoji(fish_name),
                        "water_temp": water_temp,
                        "tide": tide,
                    })
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è Âπ≥Á£ØË©≥Á¥∞„Éö„Éº„Ç∏„Ç®„É©„Éº: {e}")
                continue
    
    except Exception as e:
        print(f"  ‚ùå Âπ≥Á£ØÊµ∑„Å•„ÇäÂÖ¨Âúí„Ç®„É©„Éº: {e}")
    
    print(f"  ‚úÖ Âπ≥Á£Ø: {len(catches)}‰ª∂ÂèñÂæó")
    return {
        "name": "Âπ≥Á£ØÊµ∑„Å•„ÇäÂÖ¨Âúí",
        "area": "Á•ûÊà∏",
        "distance": 8.5,
        "info": "ÂûÇÊ∞¥Âå∫„ÉªË∂≥Â†¥ËâØÂ•Ω„ÉªÊäï„ÅíÈá£„Çä‰∫∫Ê∞ó„ÉªÈßêËªäÂ†¥„ÅÇ„Çä",
        "source": "kobeumiduri.jp",
        "catches": catches
    }


# =============================================================
# 3. „Ç´„É≥„Éë„É™ÔºàÁ•ûÊà∏Êù±ÈÉ®„ÉªÁ•ûÊà∏Ë•øÈÉ®Ôºâ
# =============================================================
def collect_kanpari():
    """„Ç´„É≥„Éë„É™„Åã„ÇâÈá£ÊûúÂèéÈõÜ"""
    print("üì° [3/5] „Ç´„É≥„Éë„É™„Åã„ÇâÂèéÈõÜ‰∏≠...")
    catches = []
    
    areas = [
        ("Á•ûÊà∏Êù±ÈÉ®", "https://fishing.ne.jp/fishingpost/area/kobe-tobu", "Á•ûÊà∏"),
        ("Á•ûÊà∏Ë•øÈÉ®", "https://fishing.ne.jp/fishingpost/area/kobe-seibu", "Á•ûÊà∏"),
        ("ÊòéÁü≥", "https://fishing.ne.jp/fishingpost/area/akashi", "ÊòéÁü≥"),
    ]
    
    for area_name, url, area_tag in areas:
        try:
            time.sleep(1)
            resp = requests.get(url, headers=HEADERS, timeout=15)
            resp.encoding = 'utf-8'
            soup = BeautifulSoup(resp.text, 'lxml')
            
            # ÊäïÁ®øË®ò‰∫ã„ÇíÂèñÂæó
            articles = soup.select('a[href*="fishingpost&p="]')
            
            for article in articles[:5]:
                title = article.select_one('h1, h2, h3')
                title_text = title.get_text(strip=True) if title else ""
                
                # Êó•‰ªòÂèñÂæó
                date_str = ""
                date_elem = article.find(string=re.compile(r'20\d{2}/\d{2}/\d{2}'))
                if date_elem:
                    m = re.search(r'(\d{4})/(\d{2})/(\d{2})', str(date_elem))
                    if m:
                        date_str = f"{int(m.group(2))}/{int(m.group(3))}"
                
                # „É¶„Éº„Ç∂„ÉºÂêç
                user_elem = article.select_one('a[href*="profile"]')
                user_name = user_elem.get_text(strip=True) if user_elem else ""
                
                # Ë™¨ÊòéÊñá
                desc = ""
                desc_elem = article.select_one('p') or article.find(string=re.compile(r'.{10,}'))
                if desc_elem:
                    desc = str(desc_elem).strip()[:100]
                
                # È≠öÁ®Æ„Çí„Çø„Ç∞„Åã„ÇâÂèñÂæó
                fish_tags = article.select('a[href*="fish="]')
                fish_name = ""
                for ft in fish_tags:
                    t = ft.get_text(strip=True).replace('Èá£„Çä', '').replace('Èá£Êûú', '')
                    if t:
                        fish_name = t
                        break
                
                # ‰ªïÊéõ„Åë„Çø„Ç∞
                method_tags = article.select('a[href*="howto="]')
                method = ""
                for mt in method_tags:
                    t = mt.get_text(strip=True).replace('Èá£Êûú', '')
                    if t:
                        method = t
                        break
                
                if not fish_name and title_text:
                    fish_name = title_text
                
                if fish_name or title_text:
                    catches.append({
                        "fish": fish_name or title_text,
                        "size": "",
                        "count": "",
                        "method": method,
                        "user": user_name,
                        "date": date_str,
                        "emoji": get_emoji(fish_name or title_text),
                        "description": desc,
                        "area_detail": area_name,
                    })
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è „Ç´„É≥„Éë„É™({area_name})„Ç®„É©„Éº: {e}")
            continue
    
    print(f"  ‚úÖ „Ç´„É≥„Éë„É™: {len(catches)}‰ª∂ÂèñÂæó")
    return {
        "name": "„Ç´„É≥„Éë„É™ÊäïÁ®ø",
        "area": "Á•ûÊà∏",
        "distance": None,
        "info": "„É¶„Éº„Ç∂„ÉºÊäïÁ®ø„ÅÆÈá£ÊûúÊÉÖÂ†±",
        "source": "fishing.ne.jp",
        "catches": catches
    }


# =============================================================
# 4. „Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éû„ÉÉ„ÇØ„ÇπÔºàGoogleÊ§úÁ¥¢ÁµåÁî±Ôºâ
# =============================================================
def collect_fishingmax():
    """„Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éû„ÉÉ„ÇØ„Çπ„ÅÆÈá£Êûú„ÇíGoogleÊ§úÁ¥¢ÁµåÁî±„ÅßÂèñÂæó"""
    print("üì° [4/5] „Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éû„ÉÉ„ÇØ„ÇπÔºàGoogleÁµåÁî±Ôºâ„Åã„ÇâÂèéÈõÜ‰∏≠...")
    catches = []
    
    try:
        # „Åæ„ÅöÁõ¥Êé•„Ç¢„ÇØ„Çª„Çπ„ÇíË©¶Ë°å
        urls_to_try = [
            "https://fishingmax.co.jp/fishingpost?shop=shop-kobeharvor",
            "https://fishingmax.co.jp/fishingpost?shop=shop-tarumi",
        ]
        
        for url in urls_to_try:
            try:
                time.sleep(1)
                resp = requests.get(url, headers=HEADERS, timeout=15)
                if resp.status_code == 200:
                    resp.encoding = 'utf-8'
                    soup = BeautifulSoup(resp.text, 'lxml')
                    
                    # Ë®ò‰∫ã„Ç´„Éº„Éâ„ÇíÂèñÂæó
                    articles = soup.select('article, .post-item, .card, [class*="post"], [class*="article"]')
                    if not articles:
                        articles = soup.select('a[href*="fishingpost/"]')
                    
                    for article in articles[:10]:
                        text = article.get_text(strip=True)
                        
                        # Êó•‰ªòÊäΩÂá∫
                        date_str = ""
                        m = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', text)
                        if m:
                            date_str = f"{int(m.group(2))}/{int(m.group(3))}"
                        
                        # „ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÈ≠öÁ®ÆÊ§úÂá∫
                        found_fish = []
                        for fish_key in FISH_EMOJI.keys():
                            if fish_key in text:
                                found_fish.append(fish_key)
                        
                        # „Çµ„Ç§„Ç∫ÊäΩÂá∫
                        size = ""
                        m = re.search(r'([\d.]+)\s*[ÔΩû~-]\s*([\d.]+)\s*[cC„éù]', text)
                        if m:
                            size = f"{m.group(1)}-{m.group(2)}cm"
                        else:
                            m = re.search(r'([\d.]+)\s*[cC„éù]', text)
                            if m:
                                size = f"~{m.group(1)}cm"
                        
                        title_elem = article.select_one('h2, h3, .title')
                        title = title_elem.get_text(strip=True)[:60] if title_elem else text[:60]
                        
                        for fish in found_fish[:2]:
                            catches.append({
                                "fish": fish,
                                "size": size,
                                "count": "",
                                "method": "",
                                "user": "",
                                "date": date_str,
                                "emoji": get_emoji(fish),
                                "description": title,
                            })
                        
                        if not found_fish and date_str:
                            catches.append({
                                "fish": title[:20],
                                "size": size,
                                "count": "",
                                "method": "",
                                "user": "",
                                "date": date_str,
                                "emoji": "üêü",
                                "description": title,
                            })
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è „Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éû„ÉÉ„ÇØ„ÇπÁõ¥Êé•„Ç¢„ÇØ„Çª„Çπ„Ç®„É©„Éº: {e}")
        
        # Áõ¥Êé•„Ç¢„ÇØ„Çª„Çπ„ÅßÂèñ„Çå„Å™„Åã„Å£„ÅüÂ†¥Âêà„ÄÅGoogleÊ§úÁ¥¢„Å´„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        if not catches:
            print("  ‚Ü™ GoogleÊ§úÁ¥¢„Å´„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ...")
            search_url = "https://www.google.com/search"
            params = {
                'q': 'site:fishingmax.co.jp Èá£Êûú Á•ûÊà∏ OR È†àÁ£® OR ÂûÇÊ∞¥',
                'num': 10,
                'tbs': 'qdr:w',  # Áõ¥Ëøë1ÈÄ±Èñì
            }
            
            try:
                time.sleep(2)
                resp = requests.get(search_url, params=params, headers=HEADERS, timeout=15)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, 'lxml')
                    
                    for result in soup.select('div.g, div[data-sokoban-container]'):
                        title_el = result.select_one('h3')
                        snippet_el = result.select_one('span, .VwiC3b')
                        
                        if title_el:
                            title = title_el.get_text(strip=True)
                            snippet = snippet_el.get_text(strip=True) if snippet_el else ""
                            combined = title + " " + snippet
                            
                            found_fish = []
                            for fish_key in FISH_EMOJI.keys():
                                if fish_key in combined:
                                    found_fish.append(fish_key)
                            
                            date_str = ""
                            m = re.search(r'(\d{1,2})/(\d{1,2})', combined)
                            if m:
                                date_str = f"{m.group(1)}/{m.group(2)}"
                            
                            for fish in found_fish[:2]:
                                catches.append({
                                    "fish": fish,
                                    "size": "",
                                    "count": "",
                                    "method": "",
                                    "user": "",
                                    "date": date_str,
                                    "emoji": get_emoji(fish),
                                    "description": title[:60],
                                })
                            
            except Exception as e:
                print(f"  ‚ö†Ô∏è GoogleÊ§úÁ¥¢„Ç®„É©„Éº: {e}")
    
    except Exception as e:
        print(f"  ‚ùå „Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éû„ÉÉ„ÇØ„Çπ„Ç®„É©„Éº: {e}")
    
    print(f"  ‚úÖ „Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éû„ÉÉ„ÇØ„Çπ: {len(catches)}‰ª∂ÂèñÂæó")
    return {
        "name": "„Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éû„ÉÉ„ÇØ„Çπ",
        "area": "Á•ûÊà∏",
        "distance": None,
        "info": "Èá£ÂÖ∑Â∫ó„ÅÆÈá£Êûú„É¨„Éù„Éº„ÉàÔºàÁ•ûÊà∏„Éè„Éº„Éê„ÉºÂ∫ó„ÉªÂûÇÊ∞¥Â∫óÔºâ",
        "source": "fishingmax.co.jp",
        "catches": catches
    }


# =============================================================
# 5. „Ç¢„É≥„Ç∞„É©„Éº„Ç∫ÔºàGoogleÊ§úÁ¥¢ÁµåÁî±Ôºâ
# =============================================================
def collect_anglers():
    """„Ç¢„É≥„Ç∞„É©„Éº„Ç∫„ÅÆÈá£Êûú„ÇíGoogleÊ§úÁ¥¢ÁµåÁî±„ÅßÂèñÂæó"""
    print("üì° [5/5] „Ç¢„É≥„Ç∞„É©„Éº„Ç∫ÔºàGoogleÁµåÁî±Ôºâ„Åã„ÇâÂèéÈõÜ‰∏≠...")
    catches = []
    
    try:
        search_url = "https://www.google.com/search"
        params = {
            'q': 'site:anglers.jp Á•ûÊà∏ OR È†àÁ£® OR Âπ≥Á£Ø OR Ëä¶Â±ã OR ÊòéÁü≥ Èá£Êûú',
            'num': 10,
            'tbs': 'qdr:w',
        }
        
        time.sleep(2)
        resp = requests.get(search_url, params=params, headers=HEADERS, timeout=15)
        
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'lxml')
            
            for result in soup.select('div.g, div[data-sokoban-container]'):
                title_el = result.select_one('h3')
                snippet_el = result.select_one('span, .VwiC3b')
                
                if title_el:
                    title = title_el.get_text(strip=True)
                    snippet = snippet_el.get_text(strip=True) if snippet_el else ""
                    combined = title + " " + snippet
                    
                    found_fish = []
                    for fish_key in FISH_EMOJI.keys():
                        if fish_key in combined:
                            found_fish.append(fish_key)
                    
                    # „Ç®„É™„Ç¢ÊäΩÂá∫
                    area = "Á•ûÊà∏"
                    for a in ['ÊòéÁü≥', 'Ëä¶Â±ã', 'È†àÁ£®', 'ÂûÇÊ∞¥', 'Âπ≥Á£Ø']:
                        if a in combined:
                            area = a
                            break
                    
                    date_str = ""
                    m = re.search(r'(\d{1,2})Êúà(\d{1,2})Êó•', combined)
                    if m:
                        date_str = f"{m.group(1)}/{m.group(2)}"
                    
                    for fish in found_fish[:2]:
                        catches.append({
                            "fish": fish,
                            "size": "",
                            "count": "",
                            "method": "",
                            "user": "",
                            "date": date_str,
                            "emoji": get_emoji(fish),
                            "description": title[:60],
                            "area_detail": area,
                        })
        
        # GoogleÊ§úÁ¥¢„Åå„Éñ„É≠„ÉÉ„ÇØ„Åï„Çå„ÅüÂ†¥Âêà„ÄÅ„Ç¢„É≥„Ç∞„É©„Éº„Ç∫ÂÖµÂ∫´Áúå„Éö„Éº„Ç∏„ÇíË©¶Ë°å
        if not catches:
            print("  ‚Ü™ „Ç¢„É≥„Ç∞„É©„Éº„Ç∫ÂÖµÂ∫´Áúå„Éö„Éº„Ç∏„ÇíÁõ¥Êé•Ë©¶Ë°å...")
            try:
                time.sleep(1)
                url = "https://anglers.jp/prefectures/28/catches"
                resp = requests.get(url, headers=HEADERS, timeout=15)
                if resp.status_code == 200 and len(resp.text) > 500:
                    soup = BeautifulSoup(resp.text, 'lxml')
                    
                    for card in soup.select('[class*="catch"], [class*="card"], article'):
                        text = card.get_text(strip=True)
                        found_fish = []
                        for fish_key in FISH_EMOJI.keys():
                            if fish_key in text:
                                found_fish.append(fish_key)
                        
                        for fish in found_fish[:1]:
                            catches.append({
                                "fish": fish,
                                "size": "",
                                "count": "",
                                "method": "",
                                "user": "",
                                "date": "",
                                "emoji": get_emoji(fish),
                                "description": text[:60],
                            })
            except Exception as e:
                print(f"  ‚ö†Ô∏è „Ç¢„É≥„Ç∞„É©„Éº„Ç∫Áõ¥Êé•„Ç¢„ÇØ„Çª„Çπ„Ç®„É©„Éº: {e}")
    
    except Exception as e:
        print(f"  ‚ùå „Ç¢„É≥„Ç∞„É©„Éº„Ç∫„Ç®„É©„Éº: {e}")
    
    print(f"  ‚úÖ „Ç¢„É≥„Ç∞„É©„Éº„Ç∫: {len(catches)}‰ª∂ÂèñÂæó")
    return {
        "name": "„Ç¢„É≥„Ç∞„É©„Éº„Ç∫",
        "area": "ÂÖµÂ∫´",
        "distance": None,
        "info": "Èá£„ÇäSNS„ÅÆ„É¶„Éº„Ç∂„ÉºÊäïÁ®ø",
        "source": "anglers.jp",
        "catches": catches
    }


# =============================================================
# ÊΩÆÊ±ê„ÉªÂ§©Êñá„Éá„Éº„ÇøË®àÁÆó
# =============================================================
def calculate_moon_phase():
    """ÊúàÈΩ¢„ÇíË®àÁÆó"""
    now = datetime.now()
    # Á∞°ÊòìÊúàÈΩ¢Ë®àÁÆóÔºà„Éñ„É©„Ç¶„É≥„ÅÆËøë‰ººÂºèÔºâ
    year = now.year
    month = now.month
    day = now.day
    
    if month <= 2:
        year -= 1
        month += 12
    
    a = int(year / 100)
    b = 2 - a + int(a / 4)
    jd = int(365.25 * (year + 4716)) + int(30.6001 * (month + 1)) + day + b - 1524.5
    
    # ÊúàÈΩ¢Ë®àÁÆó
    moon_age = (jd - 2451550.1) % 29.530588853
    moon_age = round(moon_age, 1)
    
    # Êúà„ÅÆÂêçÂâç
    if moon_age < 1.84566:
        name, icon = "Êñ∞Êúà", "üåë"
    elif moon_age < 5.53699:
        name, icon = "‰∏âÊó•Êúà", "üåí"
    elif moon_age < 9.22831:
        name, icon = "‰∏äÂº¶„ÅÆÊúà", "üåì"
    elif moon_age < 12.91963:
        name, icon = "ÂçÅ‰∏âÂ§ú", "üåî"
    elif moon_age < 16.61096:
        name, icon = "Ê∫ÄÊúà", "üåï"
    elif moon_age < 20.30228:
        name, icon = "ÂçÅÂÖ´Â§ú", "üåñ"
    elif moon_age < 23.99361:
        name, icon = "‰∏ãÂº¶„ÅÆÊúà", "üåó"
    elif moon_age < 27.68493:
        name, icon = "‰∫åÂçÅÂÖ≠Â§ú", "üåò"
    else:
        name, icon = "Êñ∞Êúà", "üåë"
    
    # ÊΩÆÂêçÔºàÁ∞°ÊòìË®àÁÆóÔºâ
    if moon_age <= 2 or (13.5 <= moon_age <= 16.5) or moon_age >= 28:
        tide = "Â§ßÊΩÆ"
    elif (3 <= moon_age <= 5) or (17 <= moon_age <= 19):
        tide = "‰∏≠ÊΩÆ"
    elif (5 < moon_age <= 7) or (19 < moon_age <= 21):
        tide = "‰∏≠ÊΩÆ"
    elif (7 < moon_age <= 9) or (21 < moon_age <= 23):
        tide = "Â∞èÊΩÆ"
    elif (9 < moon_age <= 10.5) or (23 < moon_age <= 24.5):
        tide = "Èï∑ÊΩÆ"
    else:
        tide = "Ëã•ÊΩÆ"
    
    return {
        "age": moon_age,
        "name": name,
        "icon": icon,
        "tide": tide,
    }


def calculate_sun_times():
    """Êó•„ÅÆÂá∫„ÉªÊó•„ÅÆÂÖ•„ÇäÊôÇÂàª„ÅÆËøë‰ººË®àÁÆóÔºàÁ•ûÊà∏Ôºâ"""
    now = datetime.now()
    day_of_year = now.timetuple().tm_yday
    
    # Á•ûÊà∏ÔºàÂåóÁ∑Ø34.69, Êù±Áµå135.19Ôºâ„Åß„ÅÆÁ∞°ÊòìË®àÁÆó
    lat = 34.69
    
    # Ëµ§Á∑Ø„ÅÆËøë‰ºº
    declination = -23.44 * math.cos(math.radians(360/365 * (day_of_year + 10)))
    
    # Êó•„ÅÆÂá∫„ÉªÊó•„ÅÆÂÖ•„ÇäÊôÇËßí
    cos_hour_angle = (-0.01454 - math.sin(math.radians(lat)) * math.sin(math.radians(declination))) / \
                     (math.cos(math.radians(lat)) * math.cos(math.radians(declination)))
    
    if -1 <= cos_hour_angle <= 1:
        hour_angle = math.degrees(math.acos(cos_hour_angle))
        
        # UTCÊôÇÂàª„ÇíË®àÁÆó„Åó„ÄÅJST„Å´Â§âÊèõ
        noon_offset = 12 - (135.19 / 15)  # ÁµåÂ∫¶Ë£úÊ≠£
        sunrise_utc = 12 - hour_angle / 15 + noon_offset
        sunset_utc = 12 + hour_angle / 15 + noon_offset
        
        sunrise_jst = sunrise_utc + 9
        sunset_jst = sunset_utc + 9
        
        sunrise_h = int(sunrise_jst)
        sunrise_m = int((sunrise_jst - sunrise_h) * 60)
        sunset_h = int(sunset_jst)
        sunset_m = int((sunset_jst - sunset_h) * 60)
        
        sunrise = f"{sunrise_h:02d}:{sunrise_m:02d}"
        sunset = f"{sunset_h:02d}:{sunset_m:02d}"
    else:
        sunrise = "06:30"
        sunset = "17:30"
    
    return sunrise, sunset


def calculate_mazume(sunrise, sunset):
    """„Åæ„Åö„ÇÅÊôÇÈñì„ÇíË®àÁÆó"""
    # Êúù„Åæ„Åö„ÇÅÔºöÊó•„ÅÆÂá∫30ÂàÜÂâç„ÄúÊó•„ÅÆÂá∫30ÂàÜÂæå
    sr_h, sr_m = map(int, sunrise.split(':'))
    sr_total = sr_h * 60 + sr_m
    am_start_total = sr_total - 30
    am_end_total = sr_total + 30
    
    am_start = f"{am_start_total // 60:02d}:{am_start_total % 60:02d}"
    am_end = f"{am_end_total // 60:02d}:{am_end_total % 60:02d}"
    
    # Â§ï„Åæ„Åö„ÇÅÔºöÊó•„ÅÆÂÖ•„Çä30ÂàÜÂâç„ÄúÊó•„ÅÆÂÖ•„Çä30ÂàÜÂæå
    ss_h, ss_m = map(int, sunset.split(':'))
    ss_total = ss_h * 60 + ss_m
    pm_start_total = ss_total - 30
    pm_end_total = ss_total + 30
    
    pm_start = f"{pm_start_total // 60:02d}:{pm_start_total % 60:02d}"
    pm_end = f"{pm_end_total // 60:02d}:{pm_end_total % 60:02d}"
    
    return {
        "morning": f"{am_start} - {am_end}",
        "evening": f"{pm_start} - {pm_end}",
    }


# =============================================================
# „É°„Ç§„É≥Âá¶ÁêÜ
# =============================================================
def save_data(data, filename='fishing-data.json'):
    """„Éá„Éº„Çø„ÇíJSON„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò"""
    # Êó¢Â≠ò„Éá„Éº„Çø„Åå„ÅÇ„Çå„Å∞Ë™≠„ÅøËæº„Çì„Åß„Éû„Éº„Ç∏
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            existing = json.load(f)
            existing_catches = {}
            for spot in existing.get('spots', []):
                key = spot.get('source', spot.get('name', ''))
                for c in spot.get('catches', []):
                    catch_key = f"{key}_{c.get('fish','')}_{c.get('date','')}_{c.get('size','')}"
                    existing_catches[catch_key] = True
    except (FileNotFoundError, json.JSONDecodeError):
        existing_catches = {}
    
    print(f"üíæ „Éá„Éº„Çø„Çí {filename} „Å´‰øùÂ≠ò‰∏≠...")
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print("‚úÖ ‰øùÂ≠òÂÆå‰∫ÜÔºÅ")


def run():
    """ÂÖ®„Éá„Éº„ÇøÂèéÈõÜ„ÇíÂÆüË°å"""
    print("=" * 60)
    print("üé£ Á•ûÊà∏Èá£„ÇäÊÉÖÂ†± Ëá™ÂãïÂèéÈõÜ v2.0")
    print(f"üìÖ {datetime.now().strftime('%YÂπ¥%mÊúà%dÊó• %H:%M')}")
    print("=" * 60)
    
    # ÂêÑ„ÇΩ„Éº„Çπ„Åã„ÇâÂèéÈõÜ
    spots = []
    
    spot1 = collect_suma()
    spots.append(spot1)
    
    spot2 = collect_hiraiso()
    spots.append(spot2)
    
    spot3 = collect_kanpari()
    spots.append(spot3)
    
    spot4 = collect_fishingmax()
    spots.append(spot4)
    
    spot5 = collect_anglers()
    spots.append(spot5)
    
    # ÊΩÆÊ±ê„ÉªÂ§©Êñá„Éá„Éº„Çø
    moon = calculate_moon_phase()
    sunrise, sunset = calculate_sun_times()
    mazume = calculate_mazume(sunrise, sunset)
    
    now = datetime.now()
    weekday_names = ['Êúà', 'ÁÅ´', 'Ê∞¥', 'Êú®', 'Èáë', 'Âúü', 'Êó•']
    
    # ÈõÜË®à
    total_catches = sum(len(s['catches']) for s in spots)
    
    data = {
        "lastUpdated": now.isoformat(),
        "lastUpdatedDisplay": f"{now.year}Âπ¥{now.month}Êúà{now.day}Êó•({weekday_names[now.weekday()]})",
        "spots": spots,
        "tideInfo": {
            "date": f"{now.year}Âπ¥{now.month}Êúà{now.day}Êó•({weekday_names[now.weekday()]})",
            "tide": moon["tide"],
            "moonAge": moon["age"],
            "moonName": moon["name"],
            "moonIcon": moon["icon"],
            "sunrise": sunrise,
            "sunset": sunset,
            "mazume": mazume,
        },
        "stats": {
            "totalCatches": total_catches,
            "sources": 5,
            "lastCollected": now.strftime('%Y-%m-%d %H:%M'),
        }
    }
    
    save_data(data)
    
    print("=" * 60)
    print(f"üéâ „Éá„Éº„ÇøÂèéÈõÜÂÆå‰∫ÜÔºÅ ÂêàË®à {total_catches} ‰ª∂")
    for s in spots:
        print(f"  {s['source']}: {len(s['catches'])}‰ª∂")
    print("=" * 60)
    
    return data


if __name__ == "__main__":
    run() 
